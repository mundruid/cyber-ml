{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from read_pcaps import pcap_to_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a variable that allows you to read prior saved pkl files\n",
    "READ_FROM_PKL = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mirai_df = pd.read_pickle(\"../data/blog_eda/mirai.pkl\")\n",
    "benign_df = pd.read_pickle(\"../data/blog_eda/benign.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protocol Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protocols = [\n",
    "    \"ARP\",\n",
    "    \"HTTP\",\n",
    "    \"HTTPS\",\n",
    "    \"FTP\",\n",
    "    \"FTPS\",\n",
    "    \"SMTP\",\n",
    "    \"POP3\",\n",
    "    \"IMAP\",\n",
    "    \"Telnet\",\n",
    "    \"DNS\",\n",
    "    \"DHCP\",\n",
    "    \"SNMP\",\n",
    "    \"NTP\",\n",
    "    \"SSH\",\n",
    "    \"SMB\",\n",
    "    \"LDAP\",\n",
    "    \"SIP\",\n",
    "    \"ICMP\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_protocol(payload):\n",
    "    for protocol in protocols:\n",
    "        if protocol in payload:\n",
    "            return protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_df[\"payload_proto\"] = benign_df[\"Payload\"].apply(extract_protocol)\n",
    "mirai_df[\"payload_proto\"] = mirai_df[\"Payload\"].apply(extract_protocol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "mirai_df[\"protocol_encoded\"] = LabelEncoder().fit_transform(mirai_df[\"payload_proto\"])\n",
    "benign_df[\"protocol_encoded\"] = LabelEncoder().fit_transform(benign_df[\"payload_proto\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mirai_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Service one-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read a couple more pcaps with malware, get interesting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not READ_FROM_PKL:\n",
    "    guloader = rdpcap(\n",
    "        \"../data/blog_fe/2023-06-26-guloader-or-modiloader-style-infection-for-Remcos-RAT.pcap\"\n",
    "    )\n",
    "    picabot = rdpcap(\n",
    "        \"../data/blog_fe/2023-12-18-TA577-Pikabot-infection-with-Cobalt-Strike.pcap\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not READ_FROM_PKL:\n",
    "    guloader_df = pcap_to_dataframe(guloader)\n",
    "    picabot_df = pcap_to_dataframe(picabot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to pkl since dataframe conversion takes a long time\n",
    "if not READ_FROM_PKL:\n",
    "    guloader_df.to_pickle(\"../data/blog_fe/guloader.pkl\")\n",
    "    picabot_df.to_pickle(\"../data/blog_fe/picabot.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_protocols = {\n",
    "    1: \"ICMP\",\n",
    "    6: \"TCP\",\n",
    "    17: \"UDP\",\n",
    "    23: \"Telnet\",\n",
    "    41: \"IPv6_encapsulation\",\n",
    "    47: \"GRE\",\n",
    "    50: \"ESP\",\n",
    "    51: \"AH\",\n",
    "    53: \"DNS\",\n",
    "    58: \"ICMPv6\",\n",
    "    89: \"OSPF\",\n",
    "    132: \"SCTP\",\n",
    "    135: \"SCTP\",\n",
    "    136: \"UDPLite\",\n",
    "    137: \"NETBIOS-NS\",\n",
    "    138: \"NETBIOS-DGM\",\n",
    "    139: \"NETBIOS-SSN\",\n",
    "    143: \"IMAP\",\n",
    "    161: \"SNMP\",\n",
    "    162: \"SNMP_trap\",\n",
    "    443: \"HTTPS\",\n",
    "    514: \"Syslog\",\n",
    "    636: \"LDAPS\",\n",
    "    989: \"FTPS\",\n",
    "    993: \"IMAPS\",\n",
    "    995: \"POP3S\",\n",
    "    1080: \"SOCKS_proxy\",\n",
    "    # Add more protocols as needed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if read from pkl is enabled, read from existing object\n",
    "if READ_FROM_PKL:\n",
    "    guloader_df = pd.read_pickle(\"../data/blog_fe/guloader.pkl\")\n",
    "    picabot_df = pd.read_pickle(\"../data/blog_fe/picabot.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_port(port, df):\n",
    "    new_df = pd.DataFrame()\n",
    "    for protocol_port, protocol_name in network_protocols.items():\n",
    "        new_df[protocol_name] = df[port].apply(\n",
    "            lambda port: 1 if port == protocol_port else 0\n",
    "        )\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guloader_protocol_one_hot = one_hot_port(\"Destination Port\", guloader_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guloader_protocol_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picabot_protocol_one_hot = one_hot_port(\"Destination Port\", picabot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picabot_protocol_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packet length ordinal encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_ordinal_mapping(length, low, medium, high):\n",
    "    if length <= low:\n",
    "        return \"LOW\"\n",
    "    elif low < length <= medium:\n",
    "        return \"MEDIUM\"\n",
    "    return \"HIGH\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guloader_df[\"Risk\"] = guloader_df[\"Packet Length\"].apply(\n",
    "    lambda x: length_ordinal_mapping(x, 60, 256, 1024)\n",
    ")\n",
    "picabot_df[\"Risk\"] = picabot_df[\"Packet Length\"].apply(\n",
    "    lambda x: length_ordinal_mapping(x, 60, 256, 1024)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guloader_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Port frequency encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_encoding = mirai_df[\"Protocol\"].value_counts(normalize=True).to_dict()\n",
    "mirai_df[\"Protocol_freq_encoded\"] = mirai_df[\"Protocol\"].map(frequency_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mirai_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the payload column to individual words\n",
    "tokenized_payloads = mirai_df[\"Payload\"].apply(lambda x: x.lower().split())\n",
    "\n",
    "tokenized_payloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec model on tokenized payloads\n",
    "word2vec_model = Word2Vec(\n",
    "    sentences=tokenized_payloads, vector_size=100, window=5, min_count=1, workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the average word embedding for a sentence\n",
    "def average_word_embedding(sentence, model):\n",
    "    vectors = [model.wv[word] for word in sentence if word in model.wv]\n",
    "    # fix size of embeddings if it is variable\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# List of cybersecurity words\n",
    "cybersecurity_words = [\n",
    "    \"firewall\",\n",
    "    \"malware\",\n",
    "    \"phishing\",\n",
    "    \"encryption\",\n",
    "    \"authentication\",\n",
    "    \"vulnerability\",\n",
    "    \"patch\",\n",
    "    \"incident\",\n",
    "    \"antivirus\",\n",
    "    \"cryptography\",\n",
    "    \"update\",\n",
    "    \"spyware\",\n",
    "    \"verification\",\n",
    "]\n",
    "\n",
    "\n",
    "# Function to get the word embeddings\n",
    "def get_word_embeddings(word_list):\n",
    "    embeddings = {}\n",
    "    for word in word_list:\n",
    "        embeddings[word] = nlp(word).vector\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# Get word embeddings for the cybersecurity words\n",
    "word_embeddings = get_word_embeddings(cybersecurity_words)\n",
    "\n",
    "# Print the word embeddings\n",
    "for word, embedding in word_embeddings.items():\n",
    "    print(f\"{word}: {embedding}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate cosine similarity between embeddings\n",
    "def calculate_cosine_similarity(embeddings, dictionary):\n",
    "    if dictionary:\n",
    "        similarities = {}\n",
    "    else:\n",
    "        similarities = np.zeros((len(embeddings), len(embeddings)))\n",
    "    for i, word1 in enumerate(embeddings):\n",
    "        for j, word2 in enumerate(embeddings):\n",
    "            if dictionary:\n",
    "                if word1 != word2:\n",
    "                    similarity = cosine_similarity(\n",
    "                        [embeddings[word1]], [embeddings[word2]]\n",
    "                    )[0][0]\n",
    "                    similarities[(word1, word2)] = similarity\n",
    "            else:\n",
    "                if i != j:\n",
    "                    similarity = cosine_similarity(\n",
    "                        [embeddings[word1]], [embeddings[word2]]\n",
    "                    )[0][0]\n",
    "                    similarities[i, j] = similarity\n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get word embeddings for the cybersecurity words\n",
    "word_embeddings = {word: nlp(word).vector for word in cybersecurity_words}\n",
    "\n",
    "# Calculate cosine similarities\n",
    "similarities = calculate_cosine_similarity(word_embeddings, 1)\n",
    "\n",
    "# Print the cosine similarities\n",
    "for pair, similarity in similarities.items():\n",
    "    print(f\"Cosine Similarity between '{pair[0]}' and '{pair[1]}': {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cosine similarities arrays for plotting\n",
    "similarities = calculate_cosine_similarity(word_embeddings, 0)\n",
    "\n",
    "# Plot the heatmap\n",
    "sns.set(font_scale=1.2)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(similarities, annot=True, xticklabels=cybersecurity_words, yticklabels=cybersecurity_words, cmap=\"YlGnBu\")\n",
    "plt.title(\"Cosine Similarities between Cybersecurity Words\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the average_word_embedding function to create a new column 'payload_embedding'\n",
    "mirai_df[\"payload_embedding\"] = tokenized_payloads.apply(\n",
    "    lambda x: average_word_embedding(x, word2vec_model)\n",
    ")\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "mirai_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Universal Sentence Encoder\n",
    "use_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "use_embed = hub.load(use_url)\n",
    "\n",
    "\n",
    "# Function to generate embeddings for payloads\n",
    "def generate_embeddings(payloads):\n",
    "    embeddings = use_embed(payloads).numpy()\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for the \"payload\" column for a subset of mirai because it takes tooooooo looooong\n",
    "mirai_df_subset = mirai_df.head(1000)\n",
    "mirai_df_subset[\"payload_embedding_tensorflow\"] = mirai_df_subset[\"Payload\"].apply(\n",
    "    lambda x: generate_embeddings([x])\n",
    ")\n",
    "\n",
    "mirai_df_subset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
