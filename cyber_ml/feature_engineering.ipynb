{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "scan = pd.read_csv(\"../data/blog_eda/scan.csv\")\n",
    "scan.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mirai = pd.read_csv(\"../data/blog_eda/mirai.csv\")\n",
    "\n",
    "mirai.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_packets(df, column):\n",
    "    packets = df[column].value_counts()\n",
    "    return packets\n",
    "\n",
    "\n",
    "def count_bytes(df, column):\n",
    "    total_bytes = df.groupby(column)[\"Length\"].sum()\n",
    "    return total_bytes\n",
    "\n",
    "\n",
    "def count_packets_stream(df, source_ip, source_port):\n",
    "    return df[\n",
    "        (df[\"source ip\"] == source_ip) & (df[\"source port\"] == source_port)\n",
    "    ].shape[0]\n",
    "\n",
    "\n",
    "count_bytes(mirai, \"Source\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stream aggregation:\n",
    "- H: Stats summarizing the recent traffic from this packet's host (IP)\n",
    "- HH: Stats summarizing the recent traffic going from this packet's host (IP) to the packet's destination host.\n",
    "- HpHp: Stats summarizing the recent traffic going from this packet's host+port (IP) to the packet's destination host+port. Example 192.168.4.2:1242 -> 192.168.4.12:80\n",
    "- HH_jit: Stats summarizing the jitter of the traffic going from this packet's host (IP) to the packet's destination host."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_streams(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Create an empty list to store stream data as separate dataframes\n",
    "    dfs = []\n",
    "\n",
    "    # Group packets by src/dst IP and src/dst port\n",
    "    grouped = df.groupby(\n",
    "        [\"Source IP\", \"Destination IP\", \"Source Port\", \"Destination Port\", \"Protocol\"]\n",
    "    )\n",
    "\n",
    "    # Iterate through each group to extract stream data\n",
    "    for name, group in grouped:\n",
    "        # Get source/destination IP, port, and protocol\n",
    "        src_ip, dst_ip, src_port, dst_port, proto = name\n",
    "\n",
    "        # Get number of packets, total length, and duration of the stream\n",
    "        num_packets = len(group)\n",
    "        total_length = group[\"Packet Length\"].sum()\n",
    "        start_time = group[\"Timestamp\"].min()\n",
    "        end_time = group[\"Timestamp\"].max()\n",
    "        duration = float(end_time - start_time)\n",
    "\n",
    "        # Create a new dataframe with the stream data\n",
    "        stream_df = pd.DataFrame(\n",
    "            {\n",
    "                \"Source IP\": [src_ip],\n",
    "                \"Destination IP\": [dst_ip],\n",
    "                \"Source Port\": [src_port],\n",
    "                \"Destination Port\": [dst_port],\n",
    "                \"Protocol\": [proto],\n",
    "                \"Number of Packets\": [num_packets],\n",
    "                \"Total Length\": [total_length],\n",
    "                \"Duration\": [duration],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Add the new dataframe to the list\n",
    "        dfs.append(stream_df)\n",
    "\n",
    "    # Concatenate all the dataframes in the list into one dataframe\n",
    "    stream_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # Return the new dataframe with stream data\n",
    "    return stream_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mirai_stream_df = extract_streams(mirai_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical\n",
    "- one hot encoding for ports\n",
    "- word2vec encoding for payload (add payload to `stream_df`)\n",
    "\n",
    "TODO: moved from bsides talk, need to use wordvec for raw data and not streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add back payload after all the numeric EDA\n",
    "mirai_stream_df_engineered = mirai_stream_df_numeric.join(mirai_df[\"Payload\"])\n",
    "benign_stream_df_engineered = benign_stream_df_numeric.join(mirai_df[\"Payload\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sentences from payload to use in vectorization\n",
    "mirai_sentences = [simple_preprocess(payload) for payload in mirai_df[\"Payload\"]]\n",
    "benign_sentences = [simple_preprocess(payload) for payload in benign_df[\"Payload\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mirai_model = Word2Vec(sentences=mirai_sentences, window=5, min_count=1, workers=4)\n",
    "benign_model = Word2Vec(sentences=benign_sentences, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mirai_stream_df_engineered[\"Payload_vectors\"] = mirai_stream_df_engineered[\n",
    "    \"Payload\"\n",
    "].apply(\n",
    "    lambda payload: mirai_model.wv[simple_preprocess(payload)]\n",
    "    if simple_preprocess(payload)\n",
    "    else None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_preprocess(payload, model):\n",
    "    try:\n",
    "        if simple_preprocess(payload):\n",
    "            return model.wv[simple_preprocess(payload)]\n",
    "        else:\n",
    "            return None\n",
    "    except KeyError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_stream_df_engineered[\"Payload_vectors\"] = benign_stream_df_engineered[\n",
    "    \"Payload\"\n",
    "].apply(lambda payload: try_preprocess(payload, benign_model))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
